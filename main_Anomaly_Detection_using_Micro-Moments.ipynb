{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595715601088",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifer_metrics(y_true, y_pred):\n",
    "    classifer_accuracy = accuracy_score(y_true, y_pred)*100\n",
    "    classifier_f1_score = f1_score(y_true, y_pred, average='macro')*100\n",
    "    return classifer_accuracy, classifier_f1_score\n",
    "\n",
    "# This line reads the dataset names instead of hardcoding them.\n",
    "datasets = os.listdir('./datasets')\n",
    "iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SVM\n",
    "training_time_SVM = {k:[] for k in datasets}\n",
    "training_accuracy_SVM = {k:[] for k in datasets}\n",
    "training_f1_score_SVM = {k:[] for k in datasets}\n",
    "testing_time_SVM = {k:[] for k in datasets}\n",
    "testing_accuracy_SVM = {k:[] for k in datasets}\n",
    "testing_f1_score_SVM = {k:[] for k in datasets}\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"------------- {dataset[:-4]} Execution -------------\")\n",
    "    with open(f'./datasets/{dataset}','r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        data = list(csv_reader)\n",
    "\n",
    "    data = np.array(data)\n",
    "    data = data.astype(np.float)\n",
    "        \n",
    "    num_observations = data.shape[0]\n",
    "    num_features = data.shape[1] - 1\n",
    "\n",
    "    print(f\"Dataset Size: {data.shape}\\n\")\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Shuffle the data\n",
    "        shuffle_idx = np.random.permutation(num_observations)\n",
    "        shuffled_data = data[shuffle_idx,:]\n",
    "\n",
    "        X = shuffled_data[:,:-1]\n",
    "        y = shuffled_data[:,-1]\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "        \n",
    "        classifier = SVC(kernel = 'poly', random_state = 0)\n",
    "\n",
    "        training_time_start = time.time()\n",
    "        \n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_train_pred = classifier.predict(X_train)\n",
    "        classifir_accuracy, classifier_f1_score = classifer_metrics(y_train, y_train_pred)\n",
    "        training_accuracy_SVM[dataset].append(classifir_accuracy)\n",
    "        training_f1_score_SVM[dataset].append(classifier_f1_score)\n",
    "\n",
    "        training_time_end = time.time()\n",
    "        training_time_SVM[dataset].append(training_time_end - training_time_start)\n",
    "\n",
    "        testing_time_start = time.time()\n",
    "        \n",
    "        y_test_pred = classifier.predict(X_test)\n",
    "        classifir_accuracy, classifier_f1_score = classifer_metrics(y_test, y_test_pred)\n",
    "        testing_accuracy_SVM[dataset].append(classifir_accuracy)\n",
    "        testing_f1_score_SVM[dataset].append(classifier_f1_score)\n",
    "        \n",
    "        testing_time_end = time.time()\n",
    "        testing_time_SVM[dataset].append(testing_time_end - testing_time_start)\n",
    "\n",
    "        print(f\"Training Time_{i+1}: --- {training_time_SVM[dataset][i]:.4f} seconds ---\")\n",
    "        print(f\"Training Accuracy_{i+1}: --- {training_accuracy_SVM[dataset][i]:.2f}%\")\n",
    "        print(f\"Training F1 Score_{i+1}: --- {training_f1_score_SVM[dataset][i]:.2f}%\")\n",
    "        print(f\"Testing Time_{i+1}: --- {testing_time_SVM[dataset][i]:.4f} seconds ---\")\n",
    "        print(f\"Testing Accuracy_{i+1}: --- {testing_accuracy_SVM[dataset][i]:.2f}%\")\n",
    "        print(f\"Testing F1 Score_{i+1}: --- {testing_f1_score_SVM[dataset][i]:.2f}%\")\n",
    "        print(\"----------------------------------------------\\n\")\n",
    "\n",
    "    print(f\"\\nAverage Training Time: --- {np.mean(training_time_SVM[dataset]):.4f} seconds ---\")\n",
    "    print(f\"Average Training Accuracy: --- {np.mean(training_accuracy_SVM[dataset]):.2f}%\")\n",
    "    print(f\"Average Training F1 Score: --- {np.mean(training_f1_score_SVM[dataset]):.2f}%\")\n",
    "    print(f\"Average Testing Time: --- {np.mean(testing_time_SVM[dataset]):.4f} seconds ---\")\n",
    "    print(f\"Average Testing Accuracy: --- {np.mean(testing_accuracy_SVM[dataset]):.2f}%\")\n",
    "    print(f\"Average Testing F1 Score: --- {np.mean(testing_f1_score_SVM[dataset]):.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#DNN\n",
    "training_time_DNN = {k:[] for k in datasets}\n",
    "training_accuracy_DNN = {k:[] for k in datasets}\n",
    "training_f1_score_DNN = {k:[] for k in datasets}\n",
    "testing_time_DNN = {k:[] for k in datasets}\n",
    "testing_accuracy_DNN = {k:[] for k in datasets}\n",
    "testing_f1_score_DNN = {k:[] for k in datasets}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"------------- {dataset[:-4]} Execution -------------\")\n",
    "    with open(f'./datasets/{dataset}','r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        data = list(csv_reader)\n",
    "\n",
    "    data = np.array(data)\n",
    "    data = data.astype(np.float)\n",
    "        \n",
    "    num_observations = data.shape[0]\n",
    "    num_features = data.shape[1] - 1\n",
    "\n",
    "    print(f\"Dataset Size: {data.shape}\\n\")\n",
    "    for i in range(iterations):\n",
    "        # Shuffle the data\n",
    "        shuffle_idx = np.random.permutation(num_observations)\n",
    "        shuffled_data = data[shuffle_idx,:]\n",
    "\n",
    "        X = shuffled_data[:,:-1]\n",
    "        Y = shuffled_data[:,-1]\n",
    "\n",
    "        y_ = Y.reshape(-1, 1) # Convert data to a single column\n",
    "\n",
    "        # normlize power column in dataset\n",
    "        X[:,1] = (X[:,1] - X[:,1].mean())/(X[:,1].max()-X[:,1].min())\n",
    "\n",
    "        # One Hot encode the class labels\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        Y = encoder.fit_transform(y_)\n",
    "\n",
    "        # Split the data for training and testing\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)\n",
    "\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_shape=(num_features,), activation='relu', name='fc1'))\n",
    "        model.add(Dense(10, activation='relu', name='fc2'))\n",
    "        model.add(Dense(5, activation='softmax', name='output'))\n",
    "\n",
    "        # Adam optimizer with learning rate of 0.001\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        print('Neural Network Model Summary: ')\n",
    "        print(model.summary())\n",
    "\n",
    "        # Train the model\n",
    "        training_time_start = time.time()\n",
    "\n",
    "        history = model.fit(X_train, y_train, verbose=2, batch_size=50, epochs=200)\n",
    "        y_train_pred = model.predict_classes(X_train)\n",
    "        classifir_accuracy, classifier_f1_score = classifer_metrics(np.argmax(y_train, axis=1), y_train_pred)\n",
    "        training_accuracy_DNN[dataset].append(classifir_accuracy)\n",
    "        training_f1_score_DNN[dataset].append(classifier_f1_score)\n",
    "        # training_accuracy_DNN[dataset].append(history.history.get('accuracy')[-1]*100)\n",
    "\n",
    "        training_time_end = time.time()\n",
    "        training_time_DNN[dataset].append(training_time_end - training_time_start)\n",
    "\n",
    "        # Test on unseen data\n",
    "        testing_time_start = time.time()\n",
    "        y_test_pred = model.predict_classes(X_test)\n",
    "        classifier_accuracy, classifier_f1_score = classifer_metrics(np.argmax(y_test, axis=1), y_test_pred)\n",
    "        testing_accuracy_DNN[dataset].append(classifier_accuracy)\n",
    "        testing_f1_score_DNN[dataset].append(classifier_f1_score)\n",
    "        # testing_accuracy_DNN[dataset].append(results[1]*100)\n",
    "        testing_time_end = time.time()\n",
    "        testing_time_DNN[dataset].append(testing_time_end - testing_time_start)\n",
    "\n",
    "\n",
    "        # print('Final test set loss: {:4f}'.format(results[0]))\n",
    "        print(f\"Training Time_{i+1}: --- {training_time_DNN[dataset][i]:.4f} seconds ---\")\n",
    "        print(f\"Training Accuracy_{i+1}: --- {training_accuracy_DNN[dataset][i]:.2f}%\")\n",
    "        print(f\"Training F1 Score_{i+1}: --- {training_f1_score_DNN[dataset][i]:.2f}%\")\n",
    "        print(f\"Testing Time_{i+1}: --- {testing_time_DNN[dataset][i]:.4f} seconds ---\")\n",
    "        print(f\"Testing Accuracy_{i+1}: --- {testing_accuracy_DNN[dataset][i]:.2f}%\")\n",
    "        print(f\"Testing F1 Score_{i+1}: --- {testing_f1_score_DNN[dataset][i]:.2f}%\")\n",
    "        print(\"----------------------------------------------\\n\")\n",
    "\n",
    "    print(f\"\\nAverage Training Time: --- {np.mean(training_time_DNN[dataset]):.4f} seconds ---\")\n",
    "    print(f\"Average Training Accuracy: --- {np.mean(training_accuracy_DNN[dataset]):.2f}%\")\n",
    "    print(f\"Average Training F1 Score: --- {np.mean(training_f1_score_DNN[dataset]):.2f}%\")\n",
    "    print(f\"Average Testing Time: --- {np.mean(testing_time_DNN[dataset]):.4f} seconds ---\")\n",
    "    print(f\"Average Testing Accuracy: --- {np.mean(testing_accuracy_DNN[dataset]):.2f}%\")\n",
    "    print(f\"Average Testing F1 Score: --- {np.mean(testing_f1_score_DNN[dataset]):.2f}%\\n\")\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"----------------------------------------------\\n\")\n",
    "    print(f\"------------- {dataset[:-4]} Summary -------------\")\n",
    "    for i in range(iterations):\n",
    "        print(f\"Training Time_{i+1}: --- {training_time_DNN[dataset][i]:.4f} seconds ---\")\n",
    "        print(f\"Training Accuracy_{i+1}: --- {training_accuracy_DNN[dataset][i]:.2f}%\")\n",
    "        print(f\"Training F1 Score_{i+1}: --- {training_f1_score_DNN[dataset][i]:.2f}%\")\n",
    "        print(f\"Testing Time_{i+1}: --- {testing_time_DNN[dataset][i]:.4f} seconds ---\")\n",
    "        print(f\"Testing Accuracy_{i+1}: --- {testing_accuracy_DNN[dataset][i]:.2f}%\")\n",
    "        print(f\"Testing F1 Score_{i+1}: --- {testing_f1_score_DNN[dataset][i]:.2f}%\")\n",
    "        print(\"----------------------------------------------\\n\")\n",
    "\n",
    "    print(f\"\\nAverage Training Time: --- {np.mean(training_time_DNN[dataset]):.4f} seconds ---\")\n",
    "    print(f\"Average Training Accuracy: --- {np.mean(training_accuracy_DNN[dataset]):.2f}%\")\n",
    "    print(f\"Average Training F1 Score: --- {np.mean(training_f1_score_DNN[dataset]):.2f}%\")\n",
    "    print(f\"Average Testing Time: --- {np.mean(testing_time_DNN[dataset]):.4f} seconds ---\")\n",
    "    print(f\"Average Testing Accuracy: --- {np.mean(testing_accuracy_DNN[dataset]):.2f}%\")\n",
    "    print(f\"Average Testing F1 Score: --- {np.mean(testing_f1_score_DNN[dataset]):.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#KNN\n",
    "training_time_KNN = {k:[] for k in datasets}\n",
    "training_accuracy_KNN = {k:[] for k in datasets}\n",
    "training_f1_score_KNN = {k:[] for k in datasets}\n",
    "testing_time_KNN = {k:[] for k in datasets}\n",
    "testing_accuracy_KNN = {k:[] for k in datasets}\n",
    "testing_f1_score_KNN = {k:[] for k in datasets}\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"------------- {dataset[:-4]} Execution -------------\")\n",
    "    with open(f'./datasets/{dataset}','r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        data = list(csv_reader)\n",
    "\n",
    "    data = np.array(data)\n",
    "    data = data.astype(np.float)\n",
    "        \n",
    "    num_observations = data.shape[0]\n",
    "    num_features = data.shape[1] - 1\n",
    "\n",
    "    print(f\"Dataset Size: {data.shape}\\n\")\n",
    "    for i in range(iterations):\n",
    "        # Shuffle the data\n",
    "        shuffle_idx = np.random.permutation(num_observations)\n",
    "        shuffled_data = data[shuffle_idx,:]\n",
    "\n",
    "        X = shuffled_data[:,:-1]\n",
    "        y = shuffled_data[:,-1]\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "        # Instantiate learning model (k = 5)\n",
    "        classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "        # Fitting the model\n",
    "        training_time_start = time.time()\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_train_pred = classifier.predict(X_train)\n",
    "        classifir_accuracy, classifier_f1_score = classifer_metrics(y_train, y_train_pred)\n",
    "        training_accuracy_KNN[dataset].append(classifir_accuracy)\n",
    "        training_f1_score_KNN[dataset].append(classifier_f1_score)\n",
    "\n",
    "        training_time_end = time.time()\n",
    "        training_time_KNN[dataset].append(training_time_end - training_time_start)\n",
    "\n",
    "        # Predicting the Test set results\n",
    "        testing_time_start = time.time()\n",
    "        \n",
    "        y_test_pred = classifier.predict(X_test)\n",
    "        classifir_accuracy, classifier_f1_score = classifer_metrics(y_test, y_test_pred)\n",
    "        testing_accuracy_KNN[dataset].append(classifir_accuracy)\n",
    "        testing_f1_score_KNN[dataset].append(classifier_f1_score)\n",
    "        \n",
    "        testing_time_end = time.time()\n",
    "        testing_time_KNN[dataset].append(testing_time_end - testing_time_start)\n",
    "\n",
    "        print(f\"Training Time_{i+1}: --- {training_time_KNN[dataset][i]:.4f} seconds ---\")\n",
    "        print(f\"Training Accuracy_{i+1}: --- {training_accuracy_KNN[dataset][i]:.2f}%\")\n",
    "        print(f\"Training F1 Score_{i+1}: --- {training_f1_score_KNN[dataset][i]:.2f}%\")\n",
    "        print(f\"Testing Time_{i+1}: --- {testing_time_KNN[dataset][i]:.4f} seconds ---\")\n",
    "        print(f\"Testing Accuracy_{i+1}: --- {testing_accuracy_KNN[dataset][i]:.2f}%\")\n",
    "        print(f\"Testing F1 Score_{i+1}: --- {testing_f1_score_KNN[dataset][i]:.2f}%\")\n",
    "        print(\"----------------------------------------------\\n\")\n",
    "\n",
    "    print(f\"\\nAverage Training Time: --- {np.mean(training_time_KNN[dataset]):.4f} seconds ---\")\n",
    "    print(f\"Average Training Accuracy: --- {np.mean(training_accuracy_KNN[dataset]):.2f}%\")\n",
    "    print(f\"Average Training F1 Score: --- {np.mean(training_f1_score_KNN[dataset]):.2f}%\")\n",
    "    print(f\"Average Testing Time: --- {np.mean(testing_time_KNN[dataset]):.4f} seconds ---\")\n",
    "    print(f\"Average Testing Accuracy: --- {np.mean(testing_accuracy_KNN[dataset]):.2f}%\")\n",
    "    print(f\"Average Testing F1 Score: --- {np.mean(testing_f1_score_KNN[dataset]):.2f}%\\n\")"
   ]
  }
 ]
}